{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import bs4\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA=\"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitions dictionary\n",
    "import os, json\n",
    "import pickle\n",
    "defdict = {}\n",
    "path_to_json = DATA + 'Definitions/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "for i in json_files:\n",
    "    with open(DATA + 'Definitions/'+i) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    defdict.update(data)\n",
    "definitions = list(defdict.keys())\n",
    "defvalues = list(defdict.values())\n",
    "\n",
    "docs = [\"Issue and Listing of Non Convertible Redeemable Preference Shares\", \"Investment Advisers\", \"Depositories and Participants\", \"Mutual Funds\", \"Employees Service\", \"Substantial Acquisition of Shares and Takeovers\", \"Appointment of Administrator and Procedure for Refunding to the Investors\", \"Prohibition of Fraudulent and Unfair Trade Practices relating to Securities Market\", \"Know Your Client Regulations\", \"Prohibition of Insider Trading\", \"Merchant Bankers\", \"Issue and Listing  of Securities Debt Instruments and Security Receipts\", \"Delisting of Equity Shares\",\"Issue of Capital And Disclosure Requirements2\", \"Foreign Venture Capital Investor\", \"Procedure for Board Meetings\", \"Custodian\", \"Ombudsman\", \"Investor Protection and Education Fund\", \"Foreign Portfolio Investors\", \"Issue of Sweat Equity\", \"Collective Investment Scheme\", \"Portfolio Managers\", \"Research Analysts\", \"Procedure for Search and Seizure\", \"Issue of Capital And Disclosure Requirements\", \"Share Based Employee Benefits\", \"Debenture Trustees\", \"Alternative Investment Funds\", \"Stock Exchanges and Clearing Corporations\", \"Self Regulatory Organisations\", \"Settlement Proceedings\", \"Issues and Listing of Muncipal Debt Securities\", \"Buy Back Of Securities2\",\"Issue and Listing of Debt Securities\", \"Infrastructure Investment Trusts\", \"Stock Brokers\", \"Listing Obligations and Disclosure Requirements\", \"Registrars to an Issue and Share Transfer Agents\", \"Real Estate Investment Trusts\", \"Intermediaries\", \"Certification of Associated Persons in the Securities Markets\", \"Credit Rating Agencies\", \"Regulatory Fee on Stock Exchanges\", \"Underwriters\", \"Buy Back Of Securities\", \"Bankers to an Issue\", \"Central Database of Market Participants\"]\n",
    "\n",
    "#Regulations\n",
    "finalclean48 = []\n",
    "with open(DATA + 'cleanedregulations48.pkl','rb') as f:\n",
    "    finalclean48 = pickle.load(f)    \n",
    "docregs = finalclean48\n",
    "\n",
    "#Topics documentwise\n",
    "finalcleantopics48 = []\n",
    "with open(DATA + 'cleanedregtopics48.pkl','rb') as f:\n",
    "    finalcleantopics48 = pickle.load(f)    \n",
    "finaltopics = finalcleantopics48\n",
    "\n",
    "#vocab definitions\n",
    "mainvocab = []\n",
    "with open(DATA + 'mainvocab.pkl','rb') as f:\n",
    "    mainvocab = pickle.load(f) \n",
    "\n",
    "vocabdef = []\n",
    "with open(DATA + 'vocabdef.pkl','rb') as f:\n",
    "    vocabdef = pickle.load(f) \n",
    "\n",
    "#Additional Documents\n",
    "\n",
    "with open(DATA + 'concept_filenames.pkl','rb') as f:\n",
    "    cfile = pickle.load(f) \n",
    "with open(DATA + 'concept_sentences.pkl','rb') as f:\n",
    "    csent = pickle.load(f) \n",
    "with open(DATA + 'concept_text.pkl','rb') as f:\n",
    "    ctext = pickle.load(f) \n",
    "\n",
    "with open(DATA + 'informal_filenames.pkl','rb') as f:\n",
    "    ifile = pickle.load(f) \n",
    "with open(DATA + 'informal_sentences.pkl','rb') as f:\n",
    "    isent = pickle.load(f) \n",
    "with open(DATA + 'informal_text.pkl','rb') as f:\n",
    "    itext = pickle.load(f) \n",
    "    \n",
    "with open(DATA + 'miscvocab.pkl', 'rb') as f:\n",
    "    miscvocab = pickle.load(f)\n",
    "    \n",
    "#Legal Case files\n",
    "with open(DATA + 'case_filenames.pkl','rb') as f:\n",
    "    lfile = pickle.load(f) \n",
    "with open(DATA + 'case_sentences.pkl','rb') as f:\n",
    "    lsent = pickle.load(f) \n",
    "with open(DATA + 'case_text.pkl','rb') as f:\n",
    "    ltext = pickle.load(f) \n",
    "    \n",
    "with open(DATA + 'casevocab.pkl', 'rb') as f:\n",
    "    casevocab = pickle.load(f)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(\n",
    "    stopwords.words('english') +\\\n",
    "    ['mm', 'section', 'subsection', 'schedule', '-PRON-', 'chapter', 'regulation', 'repealed', 'thereto','unpublishe', 'thereunder','guideline', 'reference','onus','make','Page','Securities','Exchange','India'])\n",
    "\n",
    "with open(DATA + 'glossary.json') as f:\n",
    "    glossary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryvocab(query):\n",
    "    query1 = \"\"\n",
    "    qvocab=[]\n",
    "    for i in query.split():\n",
    "        if(i not in stopwords.words()):\n",
    "            if(i != 'What' and i != 'what' and i!='How'):\n",
    "                qvocab.append(i)\n",
    "                query1 = query1 + \" \" + i\n",
    "    query = query1\n",
    "    return qvocab, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regextract(docinput, docs, docregs, mainvocab, vocabdef, query, glossary):\n",
    "    \n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    v = []\n",
    "    \n",
    "    if(docinput == 'All'):\n",
    "        rdocs = []\n",
    "        for i in glossary.keys():\n",
    "            if(i in query.lower()):\n",
    "                dval = glossary[i]\n",
    "                for j in dval:\n",
    "                    if(j not in rdocs):\n",
    "                        rdocs.append(j)\n",
    "        if(len(rdocs)>0):\n",
    "            docinput = []\n",
    "            for i in rdocs:\n",
    "                docinput.append(docs[i])\n",
    "        else:\n",
    "            docinput = ['All']\n",
    "            \n",
    "    else:\n",
    "        d = [docinput]\n",
    "        docinput = d\n",
    "    \n",
    "    for dval in docinput:\n",
    "    \n",
    "        if(dval != 'All'):\n",
    "            ind = docs.index(dval)\n",
    "\n",
    "            rules = docregs[ind]\n",
    "        \n",
    "            t=-1\n",
    "            vocab0 = []\n",
    "            for i in rules:\n",
    "                t=t+1\n",
    "                vocab0.append(i.split())\n",
    "            \n",
    "            for i in vocab0:\n",
    "                for j in i:\n",
    "                    if(len(j)>2):\n",
    "                        for m in range(len(symbols)):\n",
    "                            j = np.char.replace(j, symbols[m], '')\n",
    "                            j = np.char.replace(j, \",\", \"\")\n",
    "                            j = np.array2string(j)\n",
    "                            j = j.replace(\"'\",\"\")\n",
    "                        if(j not in v):\n",
    "                            v.append(j)\n",
    "        else:\n",
    "            rules = []\n",
    "            for i in docregs:\n",
    "                for j in i:\n",
    "                    rules.append(j)\n",
    "            vocab0 = mainvocab\n",
    "        \n",
    "            for j in vocab0:\n",
    "                if(len(j)>2):\n",
    "                    for m in range(len(symbols)):\n",
    "                        j = np.char.replace(j, symbols[m], '')\n",
    "                        j = np.char.replace(j, \",\", \"\")\n",
    "                        j = np.array2string(j)\n",
    "                        j = j.replace(\"'\",\"\")\n",
    "                    if(j not in v):\n",
    "                        v.append(j)\n",
    "    \n",
    "    if('All' not in docinput):\n",
    "        vdef = []\n",
    "        for i in v:\n",
    "            if(i in mainvocab and len(i)>2):\n",
    "                vindex = mainvocab.index(i)\n",
    "                vdef.append(vocabdef[vindex])\n",
    "    else:\n",
    "        vdef = vocabdef\n",
    "        \n",
    "    \n",
    "    return rules, v, vdef, docinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querypreprocess(query, qvocab, definitions, finaltopics):\n",
    "    if('regulations' in qvocab):\n",
    "        qvocab.remove('regulations')\n",
    "    if('rules' in qvocab):\n",
    "        qvocab.remove('rules')\n",
    "    if('rule' in qvocab):\n",
    "        qvocab.remove('rule')\n",
    "    if('chapter' in qvocab):\n",
    "        qvocab.remove('chapter')\n",
    "    if('section' in qvocab):\n",
    "        qvocab.remove('section')\n",
    "    if('sub' in qvocab):\n",
    "        qvocab.remove('sub')\n",
    "    if('SEBI' in qvocab):\n",
    "        qvocab.remove('SEBI')\n",
    "    if('means' in qvocab):\n",
    "        qvocab.remove('means')\n",
    "    if('shall' in qvocab):\n",
    "        qvocab.remove('shall')\n",
    "    if('Securities' in qvocab):\n",
    "        qvocab.remove('Securities')\n",
    "    if('Exchange' in qvocab):\n",
    "        qvocab.remove('Exchange')\n",
    "    if('pertaining' in qvocab):\n",
    "        qvocab.remove('pertaining')\n",
    "    if('India' in qvocab):\n",
    "        qvocab.remove('India')\n",
    "        \n",
    "    query = \"\"\n",
    "    for i in qvocab:\n",
    "        query = query + \" \" + i\n",
    "        \n",
    "    importantwords = []\n",
    "    for i in qvocab:\n",
    "        flag = 0\n",
    "        for j in definitions:\n",
    "            if(i in j):\n",
    "                flag = 1\n",
    "                break\n",
    "        if(flag == 1):\n",
    "            importantwords.append(i)\n",
    "\n",
    "    expansionwords = []\n",
    "    for i in qvocab:\n",
    "        flag=0\n",
    "        for j in finaltopics:\n",
    "            for k in j:\n",
    "                if(i == k):\n",
    "                    flag=1\n",
    "        if(flag==1):\n",
    "            if(i not in importantwords):\n",
    "                importantwords.append(i)\n",
    "        if(flag==0):\n",
    "            for j0 in definitions:\n",
    "                if(i in j0):\n",
    "                    expansionwords.append(j0)\n",
    "                    break\n",
    "    \n",
    "    qnew = query\n",
    "    for j in expansionwords:\n",
    "        k=0\n",
    "        for i in definitions:\n",
    "            if(i==j):\n",
    "                s = i\n",
    "                s = s + defvalues[k]\n",
    "                qnew = qnew + ' ' + s\n",
    "                break\n",
    "            k=k+1\n",
    "        else:\n",
    "            qnew = qnew + ' ' + j\n",
    "    query = qnew\n",
    "    \n",
    "    sent = nlp(query)\n",
    "    t=0\n",
    "    for token in sent:\n",
    "        if(str(token) in importantwords):\n",
    "            if(t!=0):\n",
    "                if(str(sent[t-1]) not in importantwords):\n",
    "                    importantwords.append(str(sent[t-1]))\n",
    "            if(token.tag_ == 'VB'):\n",
    "                importantwords.append(str(token))\n",
    "        t+=1\n",
    "    \n",
    "    return query, qvocab, importantwords, expansionwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulations Retrieval Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = STOPWORDS\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = stemming(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfreg(docregs, docinput, category):\n",
    "    \n",
    "    if(category == 'regulations'):\n",
    "        d = [] \n",
    "    \n",
    "        for i in docinput:\n",
    "            if(i in docs):\n",
    "                ind = docs.index(i)\n",
    "                r0 = docregs[ind]\n",
    "                d.append(r0)\n",
    "            \n",
    "        if(len(d)>0):\n",
    "            docregs = d\n",
    "\n",
    "    regstring = []\n",
    "    reverseMap = dict()\n",
    "    origregstring = [] # flattened docregs\n",
    "    for ind,i in enumerate(docregs):\n",
    "        for j in i:\n",
    "            origregstring.append(j)\n",
    "            reverseMap[j]=docs[ind]\n",
    "            data = preprocess(j)\n",
    "            regstring.append(data)       \n",
    "    \n",
    "    \n",
    "    wordregs = regstring\n",
    "    \n",
    "    t=0\n",
    "    remind = []\n",
    "    for i in regstring:\n",
    "        if(i=='\\xa0\\n' or i == ''):\n",
    "            remind.append(t)\n",
    "            regstring.remove(i)\n",
    "            wordregs.remove(wordregs[t])\n",
    "        t+=1\n",
    "        \n",
    "    origreg = []\n",
    "    t=0\n",
    "    while(t<len(origregstring)):\n",
    "        if(t not in remind):\n",
    "            origreg.append(origregstring[t])\n",
    "        t+=1\n",
    "            \n",
    "    wordregs = regstring\n",
    "    \n",
    "    DF = {}\n",
    "    wlist = []\n",
    "    for i in range(len(wordregs)):\n",
    "        tokens = regstring[i]\n",
    "        for w in tokens.split():\n",
    "            try:\n",
    "                DF[w].add(i)\n",
    "            except:\n",
    "                DF[w] = {i}\n",
    "                wlist.append(w)\n",
    "                \n",
    "    for i in DF:\n",
    "        DF[i] = len(DF[i])\n",
    "    \n",
    "    total_vocab = [x for x in DF]\n",
    "    total_vocab_size = len(DF)\n",
    "\n",
    "# TFIDF Calculation here\n",
    "    tf_idf = {}\n",
    "    N = len(regstring)\n",
    "    for i in range(N):\n",
    "        tokens = regstring[i].split()\n",
    "        counter = Counter(tokens)\n",
    "        for token in np.unique(tokens):\n",
    "            if(token in wlist):\n",
    "                tf = counter[token]/len(tokens)\n",
    "                df = DF[token]\n",
    "                idf = np.log((N+1)/(df+1))\n",
    "                try:\n",
    "                    tf_idf[token] = tf_idf[token] + tf*idf\n",
    "                except:\n",
    "                    tf_idf[token] = tf*idf                \n",
    "            \n",
    "    return DF, N, tf_idf, total_vocab, total_vocab_size, wordregs, regstring, origreg,reverseMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_vector(qvocab, N, total_vocab, DF):\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    querytokens = qvocab\n",
    "    counter = Counter(querytokens)\n",
    "    words_count = len(querytokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(querytokens):\n",
    "        if(token in DF):\n",
    "            tf = counter[token]/words_count\n",
    "            df = DF[token]\n",
    "            idf = math.log((N+1)/(df+1))\n",
    "\n",
    "            try:\n",
    "                ind = total_vocab.index(token)\n",
    "                Q[ind] = tf*idf\n",
    "            except:\n",
    "                pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query, total_vocab, N, qvocab, DF, origreg, D, importantwords,reverseMap):\n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = q_vector(qvocab, N, total_vocab, DF)\n",
    "\n",
    "    t=0\n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        t=t+1\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[::-1]\n",
    "    answers = []\n",
    "    indexs=[]\n",
    "    for i in out:\n",
    "        flag=0\n",
    "        for j in qvocab:\n",
    "            if(j in importantwords):\n",
    "                flag=flag+2\n",
    "            else:\n",
    "                flag=flag+1\n",
    "        if(flag>1):\n",
    "            answers.append([flag,origreg[i]])\n",
    "\n",
    "    answers.sort(reverse=True)\n",
    "    distances=list()\n",
    "    for ele in answers:\n",
    "        distances.append(ele[0])\n",
    "        if ele[1] in reverseMap.keys():\n",
    "            indexs.append(reverseMap[ele[1]])\n",
    "        else:\n",
    "            indexs.append(None)\n",
    "        \n",
    "    if(k<len(answers)):\n",
    "        return answers[0:k],indexs,distances\n",
    "    else:\n",
    "        return answers,indexs,distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pdb import set_trace\n",
    "# ;set_trace()\n",
    "def cosine_similarity_transformer(k, query, total_vocab, N, qvocab, DF, origreg, D, importantwords,reverseMap):\n",
    "    \n",
    "#     find between query and each origreg \n",
    "\n",
    "    d_cosines = []\n",
    "#     model_name = 'bert-base-nli-mean-tokens'\n",
    "    model_name='sentence-transformers/roberta-base-nli-stsb-mean-tokens'\n",
    "    model = SentenceTransformer(model_name)\n",
    "    query_vector = model.encode(query)\n",
    "    reg_vectors= model.encode(origreg)\n",
    "#     from pdb import set_trace;\n",
    "#     set_trace()    \n",
    "    t=0\n",
    "    for r in reg_vectors:\n",
    "        d_cosines.append(cosine_sim(query_vector, r))\n",
    "        t=t+1\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[::-1]\n",
    "    answers = []\n",
    "    indexs=[]\n",
    "    for i in out:\n",
    "        flag=0\n",
    "        for j in qvocab:\n",
    "            if(j in importantwords):\n",
    "                flag=flag+2\n",
    "            else:\n",
    "                flag=flag+1\n",
    "        if(flag>1):\n",
    "            answers.append([flag,origreg[i]])\n",
    "\n",
    "    answers.sort(reverse=True)\n",
    "    distances=list()\n",
    "    for ele in answers:\n",
    "        distances.append(ele[0])\n",
    "        if ele[1] in reverseMap.keys():\n",
    "            indexs.append(reverseMap[ele[1]])\n",
    "        else:\n",
    "            indexs.append(None)\n",
    "        \n",
    "    if(k<len(answers)):\n",
    "        return answers[0:k],indexs,distances\n",
    "    else:\n",
    "        return answers,indexs,distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg,importantwords,reverseMap):\n",
    "    D = np.zeros((N, len(total_vocab)))\n",
    "    for i in tf_idf:\n",
    "        try:\n",
    "            ind = total_vocab.index(i[1])\n",
    "            D[i[0]][ind] = tf_idf[i]\n",
    "        except:\n",
    "            pass\n",
    "    A,indexs,distances = cosine_similarity_transformer(k, query, total_vocab, N, qvocab, DF, origreg, D, importantwords,reverseMap)\n",
    "    \n",
    "    return A,indexs,distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def answermod(A, queryinput):\n",
    "    \n",
    "#     ans = []\n",
    "#     ansreg = []\n",
    "    \n",
    "#     t=0\n",
    "#     while(t<len(A)):\n",
    "#         A[t][0] = 'title'\n",
    "#         A[t][1] = [A[t][1]]\n",
    "#         t+=1\n",
    "#     t=0\n",
    "#     df = pd.DataFrame(A,columns=['title','paragraphs'])\n",
    "    \n",
    "#     cdqa_pipeline = QAPipeline(reader='./models/bert_qa.joblib')\n",
    "#     cdqa_pipeline.fit_retriever(df=df)\n",
    "#     print(\"Predicting\")\n",
    "#     prediction = cdqa_pipeline.predict(queryinput)\n",
    "#     print(\"Finished prediction\")\n",
    "#     return prediction[0], prediction[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "def answermod(A,queryinput):\n",
    "    model_name = \"deepset/roberta-base-squad2\" # roberta QnA model\n",
    "    model = pipeline(model=model_name, tokenizer=model_name, revision=\"v1.0\", task=\"question-answering\")\n",
    "    answers=list()\n",
    "    scores=list()\n",
    "    regs = [a[1] for a in A]\n",
    "    for r in regs:\n",
    "        try:\n",
    "            result=model(question = queryinput, context= r)\n",
    "            answers.append(result[\"answer\"])\n",
    "            scores.append(result[\"score\"])\n",
    "        except:\n",
    "            print(r)\n",
    "            print(\"=================\")\n",
    "    out = np.array(scores).argsort()[::-1]\n",
    "    return answers[out[0]], regs[out[0]] , scores[out[0]] # Returns the high score answer,context and the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QnAmodel(queryinput, docinput,k,category):\n",
    "    print('Preprocessing query......')\n",
    "    qvocab, query = queryvocab(queryinput)\n",
    "    if(category == 'regulations'):\n",
    "        rules, vocab, vdef, docinput = regextract(docinput, docs, docregs, mainvocab, vocabdef, queryinput, glossary)\n",
    "        query, qvocab, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('tf-idf calculation in progress.....')\n",
    "        DF, N, tf_idf, total_vocab, total_vocab_size, wordregs, regstring, origreg,reverseMap = tfidfreg(docregs,docinput,category)\n",
    "#  Can uncomment below lines to save dump\n",
    "#         with open(DATA + 'tfidf_regs.pkl','wb') as f:\n",
    "#             pickle.dump(tf_idf,f)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "        A,indexs,distances = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords,reverseMap)\n",
    "        Aold = A\n",
    "    elif(category == 'misc'):\n",
    "        \n",
    "        rules = ctext\n",
    "        rules.extend(itext)\n",
    "        \n",
    "        vocab = miscvocab\n",
    "        N = 440 \n",
    "        \n",
    "        with open(DATA + 'misc_tf_idf_new.pkl', 'rb') as f:\n",
    "            tf_idf = pickle.load(f)\n",
    "        with open(DATA + 'misc_total_vocab_new.pkl', 'rb') as f:\n",
    "            total_vocab = pickle.load(f)\n",
    "        with open(DATA + 'misc_DF_new.json') as f:\n",
    "            DF = json.load(f)\n",
    "        with open(DATA + 'misc_origreg_new.pkl', 'rb') as f:\n",
    "            origreg = pickle.load(f)\n",
    "        \n",
    "        with open(DATA + 'miscfiles_mapper.pkl','rb') as filer:\n",
    "            reverseMap = pickle.load(filer)\n",
    "\n",
    "        query, qvocab, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "        \n",
    "        A,indexs,distances = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords,reverseMap)\n",
    "        Aold = A\n",
    "        \n",
    "        sentences = csent\n",
    "        sentences.extend(isent)\n",
    "        Anew = []\n",
    "        for ans in A:\n",
    "            ind = rules.index(ans[1])\n",
    "            for sent in sentences[ind]:\n",
    "                Anew.append([ans[0],sent])\n",
    "        A = Anew\n",
    "    elif(category == 'legal case'):\n",
    "        rules = ltext\n",
    "        vocab = casevocab\n",
    "        N = 1496 \n",
    "        \n",
    "        with open(DATA + 'case_tf_idf_new.pkl', 'rb') as f:\n",
    "            tf_idf = pickle.load(f)\n",
    "        with open(DATA + 'case_total_vocab_new.pkl', 'rb') as f:\n",
    "            total_vocab = pickle.load(f)\n",
    "        with open(DATA + 'case_DF_new.json') as f:\n",
    "            DF = json.load(f)\n",
    "        with open(DATA + 'case_origreg_new.pkl', 'rb') as f:\n",
    "            origreg = pickle.load(f)\n",
    "            \n",
    "        with open(DATA + 'casefiles_mapper.pkl','rb') as filer:\n",
    "            reverseMap = pickle.load(filer)\n",
    "\n",
    "        with open(DATA + 'casefiles_sentences_new.pkl','rb') as filer:\n",
    "            case_sentences = pickle.load(filer)\n",
    "\n",
    "        query, qvocab, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "                \n",
    "        A,indexs,distances = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords,reverseMap)\n",
    "        Aold = A\n",
    "\n",
    "        Anew = []\n",
    "        for ans in A:\n",
    "            ind = rules.index(ans[1])\n",
    "            for sent in case_sentences[ind]:\n",
    "                Anew.append([ans[0],sent])\n",
    "        A = Anew\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    print('Extracting answer')\n",
    "    answer, ansreg ,confidence_score= answermod(A, queryinput)\n",
    "    print(\"The confidence score in answer is:\\n\",confidence_score)\n",
    "    return answer, ansreg, Aold,indexs,distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing query......\n",
      "tf-idf calculation in progress.....\n",
      "Extracting relevant answer regulations.......\n",
      "Extracting answer\n",
      "The confidence score in answer is:\n",
      " 0.05821599066257477\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# queryinput = \"What are the legal provisions on appointment and re-appointment of directors of a listed entity?\"\n",
    "# docinput = 'All'\n",
    "# category = 'regulations'\n",
    "\n",
    "# queryinput = \"I am a shareholder in xyz company with my total share capital being 7%. I sold a part of my share percent which reduced my holding by 3%. Will I be required to disclose about the same to under Form C to the company since my total shareholding has fallen below 5%?\"\n",
    "# docinput = 'All'\n",
    "# category = 'legal case'\n",
    "\n",
    "queryinput = \"How much amount will be provided as reward for informants in insider trading cases\"\n",
    "docinput = 'All'\n",
    "category = 'regulations'\n",
    "\n",
    "# queryinput = \"What are the key requirements for issuer companies to list under Innovators Growth Platform (IGP)?\"\n",
    "# docinput = 'All'\n",
    "# category = 'misc'\n",
    "\n",
    "k=10\n",
    "begin = time.time()\n",
    "answer,ansreg,Aold,indexs,distances = QnAmodel(queryinput,docinput,k, category)\n",
    "# answer,ansreg = QnAmodel(queryinput,docinput,k, category)\n",
    "\n",
    "end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much amount will be provided as reward for informants in insider trading cases\n",
      "\n",
      "No Reward\n",
      "\n",
      "7G. No Reward shall be made to an Informant:-\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print()\n",
    "print(answer)\n",
    "print()\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anew=list()\n",
    "for x in Aold:\n",
    "    Anew.append(x[1])\n",
    "Anew = Anew[:k]\n",
    "docnames=indexs[:k]\n",
    "dist=distances[:k]\n",
    "targets = [i for i in range(len(docnames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultgraph(targets,answers,doclinks,queryinput,distances):\n",
    "    idoc=\"results\"\n",
    "    docnumber = 2\n",
    "    \n",
    "    q = 'Results map '\n",
    "    edge_data = zip(targets, answers,doclinks,distances)\n",
    "    \n",
    "    nt = Network(height=\"100%\", width=\"100%\",heading=q)\n",
    "    nt.hrepulsion()\n",
    "        \n",
    "    nt.add_node(\"questionDot\", label='question', title=queryinput, color = \"red\")\n",
    "        \n",
    "    t=0\n",
    "    igcount=0\n",
    "    lccount=0\n",
    "    for e in edge_data:\n",
    "        dst = e[0]\n",
    "        h = e[1]\n",
    "        pdflink = e[2]\n",
    "        linker = pdflink+'.pdf'\n",
    "        if pdflink+'.docx' in ifile:\n",
    "            linker = pdflink+'.docx'\n",
    "        nt.add_node(str(dst), label=pdflink, title=h, color = \"blue\", url='./PDFS/'+linker)\n",
    "        \n",
    "        nt.add_edge(\"questionDot\", str(dst), value=8, color = \"#000000\", length=e[3])\n",
    "        \n",
    "        t=t+1\n",
    "    \n",
    "    nt.show_buttons()  # Allows for adjustments in UI\n",
    "    nt.show(idoc+\".html\") # saves it, check for show so that it is automatically opened for user\n",
    "    \n",
    "    with open(idoc+'.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "\n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "\n",
    "    t=0\n",
    "    appendind = []\n",
    "    for i in tokens1:\n",
    "        if('return' in i):\n",
    "            appendind.append(t)\n",
    "        t=t+1\n",
    "    \n",
    "    with open(\"interactive.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<appendind[-1]):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = appendind[-1]\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "\n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open(idoc+'.html', \"w\") as outf:\n",
    "        outf.write(str(soup))\n",
    "        \n",
    "    with open(idoc+'.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "    \n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "    \n",
    "    t0=0\n",
    "    while(t0<len(tokens1)):\n",
    "        if('\"edges\": {' in tokens1[t0]):\n",
    "            break\n",
    "        t0+=1\n",
    "        \n",
    "    with open(\"nodefont.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<t0):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = t0\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "    \n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open(idoc+'.html', \"w\") as outf:\n",
    "        outf.write(str(soup))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultgraph(targets,Anew,docnames,queryinput,dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example outputs from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much amount will be provided as reward for informants in insider trading cases\n",
      "\n",
      "\n",
      "The amount of the Reward, if payable\n",
      "\n",
      "\n",
      "7F.  (1)  Informants  who  are  considered  tentatively  eligible  for  a  Reward,  shall  submit  the Informant Reward Claim Form set out in Schedule E to the Board within the period specified in the intimation sent by the Board.\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
